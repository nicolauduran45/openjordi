{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "OpenJordi Column Mapper\n",
    "\n",
    "This script maps columns from downloaded grant data files to the CrossRef grant ontology.\n",
    "It follows a similar interface to fetch_data.py.\n",
    "\n",
    "Usage:\n",
    "    python map_columns.py --source SOURCE_ID     # Map columns for a specific source\n",
    "    python map_columns.py --force                # Force remapping even if mapping exists\n",
    "    python map_columns.py --all                  # Map all sources\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "import pandas as pd\n",
    "import hashlib\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "\n",
    "\n",
    "# Add project root to path to ensure imports work correctly\n",
    "# Add the 'config' directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'ontology')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'config')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'data')))\n",
    "\n",
    "sys.path.append('..')\n",
    "\n",
    "# Import configuration\n",
    "LLM_MODEL = \"gpt-4\"\n",
    "LOG_LEVEL = '1'\n",
    "LOG_FILE = 'a'\n",
    "LLM_API_KEY = None\n",
    "# Import schema definition\n",
    "\n",
    "from grant_ontology import crossref_metadata\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(\"openjordi.map_columns\")\n",
    "\n",
    "# Ensure output directories exist\n",
    "ONTOLOGY_DIR = os.path.join('..', \"ontology\")\n",
    "MAPPINGS_DIR = os.path.join(ONTOLOGY_DIR, \"mappings\")\n",
    "os.makedirs(MAPPINGS_DIR, exist_ok=True)\n",
    "\n",
    "\n",
    "\"\"\"Initialize the column mapper.\"\"\"\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "LLM_PROVIDER = 'openai'\n",
    "# Setup API key for LLM\n",
    "llm_provider = LLM_PROVIDER\n",
    "api_key = LLM_API_KEY or os.environ.get(\"OPENAI_API_KEY\") or os.environ.get(\"SAMBANOVA_API_KEY\")\n",
    "model = LLM_MODEL\n",
    "\n",
    "if not api_key:\n",
    "    logger.error(\"No API key found for LLM. Set OPENAI_API_KEY or SAMBANOVA_API_KEY in environment.\")\n",
    "    raise ValueError(\"Missing API key for language model\")\n",
    "\n",
    "# Initialize OpenAI client based on provider\n",
    "if llm_provider.lower() == \"togetherai\":\n",
    "    client = Together(\n",
    "        api_key=os.environ.get(\"TOGETHER_AI_API_KEY\"),\n",
    "    )\n",
    "else:  # Default to OpenAI\n",
    "    client = openai.OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '../data/raw/Gates Foundation/Committed Grants/latest/bmgf-grants.csv'#'../data/raw/Arcadia/latest/WebData-08-2024.csv'\n",
    "# Try multiple approaches to handle problematic CSVs\n",
    "try:\n",
    "    df = pd.read_csv(file_path, skip_blank_lines=True)\n",
    "except Exception as e:\n",
    "    logger.warning(f\"Standard CSV reading failed: {str(e)}\")\n",
    "    # Try with error handling options\n",
    "    try:\n",
    "        if pd.__version__ >= '1.3.0':\n",
    "            df = pd.read_csv(file_path, sep=';',on_bad_lines='skip',skip_blank_lines=True, header=1)\n",
    "        else:\n",
    "            df = pd.read_csv(file_path, error_bad_lines=False,skip_blank_lines=True,skiprows=1)\n",
    "    except Exception:\n",
    "        # Last resort: try with different encoding\n",
    "        df = pd.read_csv(file_path, encoding='latin1', \n",
    "                        on_bad_lines='skip' if pd.__version__ >= '1.3.0' else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_path, sep=',',on_bad_lines='skip',skip_blank_lines=True, header=1)\n",
    "df = df.dropna(how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_name = 'Gates Foundation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Use an LLM to map source columns to CrossRef schema.\n",
    "\n",
    "Args:\n",
    "    columns (list): List of column names to map\n",
    "    source_name (str): The name of the source\n",
    "    \n",
    "Returns:\n",
    "    dict: Mapping from source columns to schema columns\n",
    "\"\"\"\n",
    "\n",
    "columns = df.columns\n",
    "column_examples = {}\n",
    "\n",
    "for col in columns:\n",
    "    if col in df.columns:\n",
    "        if not df[col].iloc[0] is None and not pd.isna(df[col].iloc[0]):\n",
    "            column_examples[col] = str(df[col].iloc[0])\n",
    "        else:\n",
    "            break\n",
    "\n",
    "# Format column information with examples\n",
    "column_info = []\n",
    "for col in columns:\n",
    "    if col in column_examples:\n",
    "        column_info.append(f'* \"{col}\": {column_examples.get(col)}')\n",
    "    else:\n",
    "        column_info.append(f'* \"{col}\"')\n",
    "\n",
    "column_text = \"\\n\".join(column_info)\n",
    "\n",
    "\n",
    "# Prepare schema information with descriptions\n",
    "schema_info = []\n",
    "for field, details in crossref_metadata.items():\n",
    "    schema_info.append(f\"- {field}: {details['Description']} ({details['Limits']})\")\n",
    "\n",
    "schema_text = \"\\n\".join(schema_info)\n",
    "\n",
    "# Create prompt for LLM\n",
    "prompt = f\"\"\"\n",
    "You are an expert in data schema mapping for academic grant data.\n",
    "\n",
    "I need to map columns from a dataset about research grants funded by '{source_name}' to the CrossRef grant metadata schema.\n",
    "\n",
    "SOURCE COLUMNS (with examples from first row if available):\n",
    "{column_text}\n",
    "\n",
    "TARGET SCHEMA (CrossRef grant metadata):\n",
    "{schema_text}\n",
    "\n",
    "For each source column, map it to the most appropriate CrossRef schema field, or 'null' if there is no appropriate match.\n",
    "Consider semantic meaning, not just exact name matches. Be thorough and consider all possible mappings.\n",
    "\n",
    "Return your response as a valid json with the following format:\n",
    "{{\n",
    "    \"source_column_name\": \"crossref_field_name\",\n",
    "    ...\n",
    "}}\n",
    "\n",
    "Only include the Python dictionary in your response, with no additional text.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_llm_response(response_text):\n",
    "    \"\"\"\n",
    "    Clean the LLM response to ensure it's a valid Python dictionary.\n",
    "    \n",
    "    Args:\n",
    "        response_text (str): The raw response from the LLM\n",
    "        \n",
    "    Returns:\n",
    "        str: Cleaned response text that can be safely evaluated\n",
    "    \"\"\"\n",
    "    # Remove markdown code blocks if present\n",
    "    if response_text.startswith(\"```json\"):\n",
    "        response_text = response_text[9:]\n",
    "    elif response_text.startswith(\"```\"):\n",
    "        response_text = response_text[3:]\n",
    "    \n",
    "    if response_text.endswith(\"```\"):\n",
    "        response_text = response_text[:-3]\n",
    "    \n",
    "    response_text = response_text.strip()\n",
    "    \n",
    "    # Ensure it starts and ends with curly braces\n",
    "    if not response_text.startswith(\"{\"):\n",
    "        response_text = \"{\" + response_text\n",
    "    if not response_text.endswith(\"}\"):\n",
    "        response_text = response_text + \"}\"\n",
    "    \n",
    "    return response_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "model=model,\n",
    "messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful academic data assistant that maps columns between schemas.\"},\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "],\n",
    "temperature=0.1,\n",
    "max_tokens=1500,\n",
    ")\n",
    "\n",
    "# Get the response content\n",
    "response_text = response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"GRANT ID\": \"award-number\",\n",
      "    \"GRANTEE\": \"funder-name\",\n",
      "    \"PURPOSE\": \"description\",\n",
      "    \"DIVISION\": \"funding-scheme\",\n",
      "    \"DATE COMMITTED\": \"award-start-date\",\n",
      "    \"DURATION (MONTHS)\": null,\n",
      "    \"AMOUNT COMMITTED\": \"award-amount\",\n",
      "    \"GRANTEE WEBSITE\": \"resource\",\n",
      "    \"GRANTEE CITY\": null,\n",
      "    \"GRANTEE STATE\": null,\n",
      "    \"GRANTEE COUNTRY\": \"@country\",\n",
      "    \"REGION SERVED\": null,\n",
      "    \"TOPIC\": \"project-title\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(_clean_llm_response(response_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are an expert in data schema mapping for academic grant data.\n",
      "\n",
      "I need to map columns from a dataset about research grants from 'Sao Paolo' to the CrossRef grant metadata schema.\n",
      "\n",
      "SOURCE COLUMNS (with examples from first row if available):\n",
      "* \"GRANT ID\": INV-002690\n",
      "* \"GRANTEE\": World Health Organization\n",
      "* \"PURPOSE\": to reduce cholera's disease burden in both epidemic and endemic settings through use of evidence-based practices, increased stakeholder collaboration and an execution plan linked to the Ending Cholera - Global Roadmap to 2030\n",
      "* \"DIVISION\": Global Health\n",
      "* \"DATE COMMITTED\": 2021-02\n",
      "* \"DURATION (MONTHS)\": 16.0\n",
      "* \"AMOUNT COMMITTED\": 1078614.0\n",
      "* \"GRANTEE WEBSITE\": http://www.who.int\n",
      "* \"GRANTEE CITY\": Geneva\n",
      "* \"GRANTEE STATE\"\n",
      "* \"GRANTEE COUNTRY\"\n",
      "* \"REGION SERVED\"\n",
      "* \"TOPIC\"\n",
      "\n",
      "TARGET SCHEMA (CrossRef grant metadata):\n",
      "- project: Container for project information. Multiple projects may be assigned to a single Grant ID. (required; multiple allowed)\n",
      "- project-title: Title of a project funded by the grant being registered (required; multiple allowed)\n",
      "- description: Used to capture an abstract or description of a project. (optional; multiple allowed)\n",
      "- @xml:lang: Use @xml:lang to identify language for each project-title or description. (optional)\n",
      "- investigators: Container for investigator information (optional)\n",
      "- person: Container for individual investigator details (at least 1 required, multiple allowed (unbounded))\n",
      "- @role: Available roles are lead_investigator, co-lead_investigator, investigator (required)\n",
      "- @start-date: Actual start date of award (optional)\n",
      "- @end-date: Actual end date of the award (optional)\n",
      "- givenName: Given or first name (optional)\n",
      "- familyName: Family or surname (optional)\n",
      "- alternateName: Alias or nickname used by the Investigator (optional)\n",
      "- affiliation: Container for affiliation information (optional, multiple allowed)\n",
      "- institution: Institution an investigator is affiliated with when associated with the project being defined. (1 allowed, use multiple affiliation groups for investigators with multiple affiliations)\n",
      "- @country: ISO 3166-1 alpha 2-letter country code, captures location (country) of affiliation (optional)\n",
      "- ROR: A ROR ID may be supplied to disambiguate affiliation information, expressed as a URL (optional)\n",
      "- ORCID: ORCID ID of the investigator, expressed as a URL (optional)\n",
      "- award-amount: Total overall amount awarded to project (optional)\n",
      "- @currency: ISO 4217 currency for value provided in award-amount (required)\n",
      "- funding: Container for funding information. Use multiple funding sections as needed to specify different funding types (required, multiple allowed)\n",
      "- @amount: Amount of funding provided by funder (optional)\n",
      "- @funding-percentage: Percentage of overall funding (optional)\n",
      "- @funding-type: Type of funding provided, values are limited (required)\n",
      "- @null-amount: Supply for projects where an award amount is missing or can’t be disclosed (optional)\n",
      "- funder-name: Name of the funder (required)\n",
      "- funder-id: Funder identifier from the Funder Registry (required)\n",
      "- funding-scheme: Scheme for grant or award as provided by the funder (optional)\n",
      "- award-dates: Container for date information (optional)\n",
      "- @planned-start-date: Planned start date of award (optional)\n",
      "- @planned-end-date: Planned end date of award (optional)\n",
      "- award-number: Funder-supplied award ID / grant number (required)\n",
      "- award-start-date: Start date of grant funding (optional)\n",
      "- relation: Relationship metadata connecting grant to other items (other grants, funded research outputs) (optional)\n",
      "- DOI: DOI being registered (required)\n",
      "- resource: URL of grant landing page (required)\n",
      "\n",
      "For each source column, map it to the most appropriate CrossRef schema field, or 'null' if there is no appropriate match.\n",
      "Consider semantic meaning, not just exact name matches. Be thorough and consider all possible mappings.\n",
      "\n",
      "Return your response as a valid json with the following format:\n",
      "{\n",
      "    \"source_column_name\": \"crossref_field_name\",\n",
      "    ...\n",
      "}\n",
      "\n",
      "Only include the Python dictionary in your response, with no additional text.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 28\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdotenv\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Add project root to path to ensure imports work correctly\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Import configuration\u001b[39;00m\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mconfig\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfig\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     29\u001b[0m     RAW_DATA_DIR, LOG_LEVEL, LOG_FILE,\n\u001b[1;32m     30\u001b[0m     LLM_PROVIDER, LLM_API_KEY\n\u001b[1;32m     31\u001b[0m )\n\u001b[1;32m     33\u001b[0m LLM_MODEL \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Import schema definition\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import sys\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "from together import Together\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Add the 'config' directory to the system path\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'ontology')))\n",
    "sys.path.append(os.path.abspath(os.path.join('..', 'data')))\n",
    "from grant_ontology import crossref_metadata  # Import schema definition\n",
    "\n",
    "# Define the data folders\n",
    "RAW_FOLDER = os.path.abspath(os.path.join(\"..\", \"data\", \"raw\"))\n",
    "CLEANED_FOLDER = \"../ontology/mappings/\"\n",
    "os.makedirs(CLEANED_FOLDER, exist_ok=True)\n",
    "\n",
    "import re\n",
    "import re\n",
    "import json\n",
    "\n",
    "def extract_json_from_text(text):\n",
    "    try:\n",
    "        # Use regex to find JSON content\n",
    "        match = re.search(r'```json\\n(.*?)\\n```', text, re.DOTALL)\n",
    "        if match:\n",
    "            json_str = match.group(1)  # Extract JSON string\n",
    "            return json.loads(json_str)  # Parse JSON\n",
    "        else:\n",
    "            return None  # No JSON found\n",
    "    except json.JSONDecodeError as e:\n",
    "        print(\"Error decoding JSON:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "def generate_prompt(example_prompt, crossref_metadata):\n",
    "  my_prompt = f'''Persona:\n",
    "You are a meticulous data analyst specializing in research funding metadata. Your expertise lies in mapping datasets to standardized schemas while ensuring high accuracy and logical consistency. You strictly adhere to the given schema constraints, avoiding any modifications or extensions beyond the defined attributes. You carefully evaluate both column names and example values to determine the most appropriate mapping.\n",
    "\n",
    "Task:\n",
    "Given the following dataset columns, map them to the closest attribute in the CrossRef metadata schema. Only select attributes that have the most similar meaning based on both column names and example values. The CrossRef schema cannot be extended.\n",
    "\n",
    "Return the mapping as a JSON object, where keys represent dataset column names and values represent the corresponding CrossRef schema attributes.\n",
    "\n",
    "Dataset:\n",
    "{str(example_prompt)}\n",
    "\n",
    "CrossRef Schema:\n",
    "{str(crossref_metadata)}\n",
    "\n",
    "Output Format:\n",
    "\n",
    "```json\n",
    "{{\n",
    "  \"DatasetColumn1\": \"CrossRefAttribute1\",\n",
    "  \"DatasetColumn2\": \"CrossRefAttribute2\",\n",
    "  ...\n",
    "}}```\n",
    "\n",
    "Ensure that:\n",
    "\n",
    "Only the most relevant attributes are mapped.\n",
    "Attributes that do not closely match are excluded.\n",
    "The response strictly follows JSON formatting.\n",
    "'''\n",
    "\n",
    "  return my_prompt\n",
    "\n",
    "\n",
    "\n",
    "def get_column_mapping(db_schema, schema):\n",
    "    \"\"\"Use LLM to generate a mapping between raw data columns and Crossref schema.\"\"\"\n",
    "\n",
    "    crossref_metadata = \"{\"+ \"\\n\".join(f'\"{key}\": \"{value[\"Description\"]}\"' for key, value in schema.items()) + \"}\"\n",
    "\n",
    "    prompt = generate_prompt(db_schema, crossref_metadata)\n",
    "\n",
    "    client = Together(\n",
    "        api_key=os.environ.get(\"TOGETHER_AI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "                    model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
    "                    messages=[{\"role\":\"system\",\"content\":prompt}],\n",
    "                    temperature=0.1,\n",
    "                    top_p=0.1,\n",
    "                    top_k=50,\n",
    "                    repetition_penalty=1,\n",
    "                    stop=[\"<|eot_id|>\",\"<|eom_id|>\"],\n",
    "                    stream=False\n",
    "                )\n",
    "\n",
    "    mapping = response.choices[0].message.content\n",
    "    return extract_json_from_text(mapping)  # Convert response string to dictionary\n",
    "\n",
    "def normalize_columns(df, mapping):\n",
    "    \"\"\"Rename columns based on LLM-provided mapping.\"\"\"\n",
    "    return df.rename(columns=mapping)\n",
    "\n",
    "def clean_data():\n",
    "    \"\"\"Clean and structure downloaded grant data based on Crossref schema.\"\"\"\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    clean_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: /home/siris/Repositories/openjordi/data/raw/HRCS/HRCS.xlsx\n",
      "✅ Cleaned data saved: ../ontology/mappings/HRCS.json\n",
      "Processing: /home/siris/Repositories/openjordi/data/raw/Swiss NSF/swiss_nsf.csv\n",
      "❌ Error processing swiss_nsf.csv: read_csv() got an unexpected keyword argument 'error_bad_lines'\n",
      "Processing: /home/siris/Repositories/openjordi/data/raw/Arcadia/arcadia.csv\n",
      "❌ Error processing arcadia.csv: read_csv() got an unexpected keyword argument 'error_bad_lines'\n"
     ]
    }
   ],
   "source": [
    "load_dotenv()\n",
    "        \n",
    "        # Setup API key for LLM\n",
    "        llm_provider = LLM_PROVIDER\n",
    "        api_key = LLM_API_KEY or os.environ.get(\"OPENAI_API_KEY\") or os.environ.get(\"SAMBANOVA_API_KEY\")\n",
    "        model = LLM_MODEL\n",
    "        \n",
    "        if not api_key:\n",
    "            logger.error(\"No API key found for LLM. Set OPENAI_API_KEY or SAMBANOVA_API_KEY in environment.\")\n",
    "            raise ValueError(\"Missing API key for language model\")\n",
    "        \n",
    "        # Initialize OpenAI client based on provider\n",
    "        if llm_provider.lower() == \"sambanova\":\n",
    "            client = openai.OpenAI(\n",
    "                api_key=api_key,\n",
    "                base_url=\"https://api.sambanova.ai/v1\",\n",
    "            )\n",
    "        else:  # Default to OpenAI\n",
    "            client = openai.OpenAI(api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Cleaned data saved: ../ontology/mappings/HRCS.json\n"
     ]
    }
   ],
   "source": [
    "cleaned_file_path = os.path.join(CLEANED_FOLDER, f\"{source_folder}.json\")\n",
    "with open(cleaned_file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(mapping, f, indent=4)\n",
    "print(f\"✅ Cleaned data saved: {cleaned_file_path}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<think>\\nAlright, I\\'m trying to map the given dataset columns to the CrossRef metadata schema. Let me go through each column one by one and see where they fit best.\\n\\nStarting with \"HRCS2022_ID\". This looks like a unique identifier for the project. In the CrossRef schema, there\\'s \"award-number\" which is the funder-supplied award ID. That seems like a perfect match.\\n\\nNext is \"FundingOrganisation\". This is the name of the organization providing the funds. The closest attribute in CrossRef is \"funder-name\", which is exactly what this column represents.\\n\\n\"FunderAcronym\" is the acronym of the funding organization, like \"MRC\" for Medical Research Council. The schema has \"funder-id\", which is an identifier from the Funder Registry. An acronym could serve as a concise identifier, so I\\'ll map this to \"funder-id\".\\n\\n\"OrganisationReference\" seems to be a specific reference number for the organization, such as \"BB/M018040/1\". This aligns with \"award-number\" as well, but since \"HRCS2022_ID\" is already mapped there, maybe it\\'s better to use \"funding-scheme\" if that\\'s about the funding scheme. Wait, \"funding-scheme\" is more about the type of grant, so perhaps \"OrganisationReference\" is more like an internal reference. Hmm, maybe it\\'s better to map it to \"award-number\" as an alternative, but since \"HRCS2022_ID\" is already there, perhaps it\\'s not necessary. Maybe it doesn\\'t have a direct match, so I might exclude it.\\n\\n\"Institution\" is the university or organization where the research is conducted. The schema has \"institution\" under \"affiliation\", so that\\'s a direct match.\\n\\n\"ROR_ID\" is a URL pointing to a ROR identifier. The schema has \"ROR\" which is a URL, so that\\'s a perfect fit.\\n\\n\"City\", \"Region\", and \"Country\" are geographical locations. The schema has \"@country\" which is a two-letter code. Since the dataset provides the full country name, maybe it\\'s better to map \"Country\" to \"@country\" and ignore \"City\" and \"Region\" as they don\\'t have direct equivalents.\\n\\n\"FundingMechanism\" describes the type of funding, like a research grant. The schema has \"@funding-type\", which is limited to specific values, so this seems appropriate.\\n\\n\"FundingStream\" might refer to a specific funding program or stream. The closest is \"funding-scheme\", which is about the grant scheme provided by the funder.\\n\\n\"StartDate\" and \"EndDate\" are dates when the award starts and ends. The schema has \"award-start-date\", \"award-dates\" with planned start and end, but since these are actual dates, \"award-start-date\" and maybe \"award-end-date\" if available. Wait, the schema doesn\\'t have \"award-end-date\", but it has \"award-dates\" with \"@planned-end-date\". Maybe \"EndDate\" can be mapped to \"@planned-end-date\" if it\\'s the actual end date. Alternatively, perhaps \"award-dates\" is a container, but since the example has both start and end, maybe it\\'s better to map \"StartDate\" to \"award-start-date\" and \"EndDate\" to \"@planned-end-date\" under \"award-dates\".\\n\\n\"Duration\" is the length of the project in days. The schema doesn\\'t have a specific attribute for duration, so this might not be mapped.\\n\\n\"TotalAward\" is the total amount awarded. This directly maps to \"award-amount\".\\n\\n\"AnnualAward\" and \"2022AnnualisedValue\" are about the yearly funding. The schema has \"@amount\" under \"funding\", which can be used for annual amounts, but since \"TotalAward\" is already mapped, maybe these can be part of \"funding\" sections with \"@amount\" and \"@funding-type\".\\n\\n\"AwardTitle\" is the title of the award, which maps to \"project-title\".\\n\\n\"AwardAbstract\" and \"AwardLayAbstract\" are descriptions. The schema has \"description\" for the project, so both can be mapped there, but perhaps only one is needed. Maybe \"AwardAbstract\" is the primary one.\\n\\n\"IndirectAward\" seems similar to the abstracts, so it might also map to \"description\", but perhaps it\\'s redundant.\\n\\nThe \"RA_\" and \"HC_\" columns with percentages are about health relevance and research areas. The schema doesn\\'t have specific attributes for these, so they might not be mapped.\\n\\n\"CodingType\", \"Direct_vs_Indirect\", \"OrganisationType\", \"HRAF\", and \"AMRC\" don\\'t have direct equivalents in the schema, so they are excluded.\\n\\nPutting it all together, I\\'ll map each relevant column to the closest CrossRef attribute, ensuring I don\\'t extend the schema beyond its defined attributes.\\n</think>\\n\\n```json\\n{\\n  \"HRCS2022_ID\": \"award-number\",\\n  \"FundingOrganisation\": \"funder-name\",\\n  \"FunderAcronym\": \"funder-id\",\\n  \"Institution\": \"institution\",\\n  \"ROR_ID\": \"ROR\",\\n  \"Country\": \"@country\",\\n  \"FundingMechanism\": \"@funding-type\",\\n  \"FundingStream\": \"funding-scheme\",\\n  \"StartDate\": \"award-start-date\",\\n  \"EndDate\": \"@planned-end-date\",\\n  \"TotalAward\": \"award-amount\",\\n  \"AwardTitle\": \"project-title\",\\n  \"AwardAbstract\": \"description\"\\n}\\n```'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # to provide this example as part of the prompt with column name and an example value\n",
    "# example_prompt = generate_prompt(df)\n",
    "\n",
    "\n",
    "\n",
    "# print(prompt)\n",
    "\n",
    "# query the model\n",
    "client = Together(\n",
    "        api_key=os.environ.get(\"TOGETHER_AI_API_KEY\"),\n",
    "    )\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "                model=\"deepseek-ai/DeepSeek-R1-Distill-Llama-70B-free\",\n",
    "                messages=[{\"role\":\"system\",\"content\":my_prompt}],\n",
    "                temperature=0.1,\n",
    "                top_p=0.1,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1,\n",
    "                stop=[\"<|eot_id|>\",\"<|eom_id|>\"],\n",
    "                stream=False\n",
    "            )\n",
    "\n",
    "mapping = response.choices[0].message.content\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.13.4-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.7-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in /home/siris/anaconda3/envs/openjordi/lib/python3.9/site-packages (from beautifulsoup4) (4.12.2)\n",
      "Downloading beautifulsoup4-4.13.4-py3-none-any.whl (187 kB)\n",
      "Downloading soupsieve-2.7-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.13.4 soupsieve-2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 2022 has ID 400\n",
    "url = \"https://www.hfsp.org/awardees/awards\"\n",
    "params = {\n",
    "    \"field_award_year_target_id[]\": 400,\n",
    "    \"page\": 1  # Page 1 = index 0\n",
    "}\n",
    "\n",
    "response = requests.get(url, params=params)\n",
    "soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "# Find all award blocks\n",
    "awards = soup.select(\".views-row\")\n",
    "\n",
    "for award in awards:\n",
    "    title = award.select_one(\".field--name-title\").get_text(strip=True)\n",
    "    print(f\"Award Title: {title}\")\n",
    "\n",
    "    # You can extract more fields like name, institution, country, etc."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "openjordi",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
